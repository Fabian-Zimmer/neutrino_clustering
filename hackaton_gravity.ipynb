{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how much memory is used by OS initially.\n",
    "# Then substract this value from later mem used, to obtain mem used by scripts.\n",
    "import psutil\n",
    "GB_UNIT = 1000*1024**2\n",
    "MB_UNIT = GB_UNIT/1e3\n",
    "OS_MEM = psutil.virtual_memory().used\n",
    "print(OS_MEM/GB_UNIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.preface import *\n",
    "import shared.functions as fct\n",
    "\n",
    "# Initialize parameters and files.\n",
    "PRE = PRE(\n",
    "    sim='L025N752', \n",
    "    z0_snap=36, z4_snap=13, DM_lim=10000,\n",
    "    sim_dir=SIM_ROOT, sim_ver=SIM_TYPE,\n",
    "    phis=20, thetas=20, vels=200,\n",
    "    pre_CPUs=128, sim_CPUs=128\n",
    ")\n",
    "\n",
    "\n",
    "TEMP_DIR = f'X_tests'\n",
    "\n",
    "Testing = False\n",
    "if Testing:\n",
    "    mass_gauge = 12.3\n",
    "    mass_range = 0.3\n",
    "    size = 1\n",
    "else:\n",
    "    mass_gauge = 12.0\n",
    "    mass_range = 0.6\n",
    "    size = 10\n",
    "\n",
    "hname = f'1e+{mass_gauge}_pm{mass_range}Msun'\n",
    "fct.halo_batch_indices(\n",
    "    PRE.Z0_STR, mass_gauge, mass_range, 'halos', size, \n",
    "    hname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "halo_batch_IDs = np.load(f'{TEMP_DIR}/halo_batch_{hname}_indices.npy')\n",
    "halo_batch_params = np.load(f'{TEMP_DIR}/halo_batch_{hname}_params.npy')\n",
    "halo_num = len(halo_batch_params)\n",
    "\n",
    "print('********Number density band********')\n",
    "print('Halo batch params (Rvir,Mvir,cNFW):')\n",
    "print(halo_batch_params)\n",
    "print('***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================== #\n",
    "# Run precalculations for selected halo in batch. #\n",
    "# =============================================== #\n",
    "\n",
    "halo_j = 2\n",
    "halo_ID = halo_batch_IDs[halo_j]\n",
    "\n",
    "# Generate progenitor index array for current halo.\n",
    "splits = re.split('/', SIM_TYPE)\n",
    "MTname = f'{PRE.SIM}_{splits[0]}_{splits[1]}'\n",
    "proj_IDs = fct.read_MergerTree(PRE.OUT_DIR, MTname, halo_ID)\n",
    "\n",
    "# Create empty arrays to save specifics of each loop.\n",
    "save_GRID_L = np.zeros(len(PRE.NUMS_SNAPS))\n",
    "save_num_DM = np.zeros(len(PRE.NUMS_SNAPS))\n",
    "save_DM_com = []\n",
    "\n",
    "# for j, (snap, proj_ID) in enumerate(zip(\n",
    "#     PRE.NUMS_SNAPS[::-1], proj_IDs\n",
    "# )):\n",
    "\n",
    "j = 0\n",
    "snap = PRE.NUMS_SNAPS[::-1][0]\n",
    "proj_ID = proj_IDs[0]\n",
    "\n",
    "proj_ID = int(proj_ID)\n",
    "\n",
    "# Output halo progress.\n",
    "print(f'halo {halo_j+1}/{halo_num} ; snapshot {snap}')\n",
    "\n",
    "\n",
    "# --------------------------- #\n",
    "# Read and load DM positions. #\n",
    "# --------------------------- #\n",
    "\n",
    "# Define how many shells are used, out of len(DM_SHELL_EDGES)-1.\n",
    "shells = 4\n",
    "DM_shell_edges = DM_SHELL_EDGES[:shells+1]\n",
    "\n",
    "IDname = f'origID{halo_ID}_snap_{snap}'\n",
    "fct.read_DM_all_inRange(\n",
    "    snap, proj_ID, DM_shell_edges, IDname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "\n",
    "# Load DM from all used shells.\n",
    "DM_pre = []\n",
    "for si in range(shells):\n",
    "    DM_pre.append(np.load(f'{TEMP_DIR}/DM_pos_{IDname}_shell{si}.npy'))\n",
    "DM_raw = np.array(list(chain.from_iterable(DM_pre)))\n",
    "del DM_pre\n",
    "\n",
    "# Save c.o.m. coord of all DM particles (for outside_gravity fct.).\n",
    "save_DM_com.append(np.sum(DM_raw, axis=0)/len(DM_raw))\n",
    "\n",
    "# ---------------------- #\n",
    "# Cell division process. #\n",
    "# ---------------------- #\n",
    "\n",
    "# Initialize grid.\n",
    "snap_GRID_L = (int(np.abs(DM_raw).max()) + 1)*kpc\n",
    "raw_grid = fct.grid_3D(snap_GRID_L, snap_GRID_L)\n",
    "init_grid = np.expand_dims(raw_grid, axis=1)\n",
    "\n",
    "# Prepare arrays for cell division.\n",
    "DM_raw *= kpc\n",
    "DM_pos = np.expand_dims(DM_raw, axis=0)\n",
    "DM_pos_for_cell_division = np.repeat(DM_pos, len(init_grid), axis=0)\n",
    "\n",
    "# Cell division.\n",
    "cell_division_count = fct.cell_division(\n",
    "    init_grid, DM_pos_for_cell_division, snap_GRID_L, PRE.DM_LIM, None, TEMP_DIR, IDname\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*(100000*3)/1.e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_gravity_short_range(\n",
    "    cell_coords_in, cell_gen, init_GRID_S,\n",
    "    DM_pos, DM_lim, DM_sim_mass, smooth_l,\n",
    "    out_dir, b_id\n",
    "):\n",
    "\n",
    "    cell_coords = np.expand_dims(np.array(cell_coords_in), axis=1)\n",
    "    cell_gen = np.array(cell_gen)\n",
    "\n",
    "    # Center all DM positions w.r.t. cell center.\n",
    "    DM_pos_sync = np.repeat(DM_pos, len(cell_coords), axis=0)\n",
    "    DM_pos_sync -= cell_coords\n",
    "\n",
    "    # Cell lengths to limit DM particles. Limit for the largest cell is \n",
    "    # GRID_S/2, not just GRID_S, therefore the cell_gen+1 !\n",
    "    cell_len = np.expand_dims(init_GRID_S/(2**(np.array(cell_gen)+1)), axis=1)\n",
    "\n",
    "    # Select DM particles inside each cell based on cube length generation.\n",
    "    DM_in_cell_IDs = np.asarray(\n",
    "        (np.abs(DM_pos_sync[:,:,0]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,1]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,2]) < cell_len)\n",
    "    )\n",
    "    del cell_gen, cell_len\n",
    "\n",
    "    # Set DM outside cell to nan values.\n",
    "    DM_pos_sync[~DM_in_cell_IDs] = np.nan\n",
    "    del DM_in_cell_IDs\n",
    "\n",
    "    # Sort all nan values to the bottom of axis 1, i.e. the DM-in-cell-X axis \n",
    "    # and truncate array based on DM_lim parameter. This simple way works since \n",
    "    # each cell cannot have more than DM_lim.\n",
    "    ind_2D = DM_pos_sync[:,:,0].argsort(axis=1)\n",
    "    ind_3D = np.repeat(np.expand_dims(ind_2D, axis=2), 3, axis=2)\n",
    "    DM_sort = np.take_along_axis(DM_pos_sync, ind_3D, axis=1)\n",
    "    DM_in = DM_sort[:,:DM_lim*SHELL_MULTIPLIERS[-1],:]\n",
    "\n",
    "    \n",
    "    # note: Memory peaks here, due to these arrays:\n",
    "    # print(DM_pos_sync.shape, ind_2D.shape, ind_3D.shape, DM_sort.shape, DM_in.shape)\n",
    "    # mem_inc = gso(cell_coords)+gso(DM_pos_sync)+gso(ind_2D)+gso(ind_3D)+gso(DM_sort)+gso(DM_in)\n",
    "    # print('MEM_PEAK:', mem_inc/1e6)\n",
    "    del DM_pos_sync, ind_2D, ind_3D, DM_sort\n",
    "\n",
    "    # Calculate distances of DM and adjust array dimensionally.\n",
    "    DM_dis = np.expand_dims(np.sqrt(np.sum(DM_in**2, axis=2)), axis=2)\n",
    "\n",
    "    # Offset DM positions by smoothening length of Camila's simulations.\n",
    "    eps = smooth_l / 2.\n",
    "\n",
    "    # nan values to 0 for numerator, and 1 for denominator to avoid infinities.\n",
    "    quot = np.nan_to_num(cell_coords - DM_in, copy=False, nan=0.0) / \\\n",
    "        np.nan_to_num(\n",
    "            np.power((DM_dis**2 + eps**2), 3./2.), copy=False, nan=1.0\n",
    "        )\n",
    "    \n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative = -G*DM_sim_mass*np.sum(quot, axis=1)    \n",
    "    np.save(f'{out_dir}/batch{b_id}_short_range.npy', derivative)\n",
    "\n",
    "\n",
    "def cell_gravity_long_range(\n",
    "    c_id, b_id, cellX_coords, \n",
    "    DM_count, cell_com, \n",
    "    DM_sim_mass, smooth_l, out_dir\n",
    "):\n",
    "\n",
    "    # Distances between cell centers and cell c.o.m. coords.\n",
    "    com_dis = np.expand_dims(\n",
    "        np.sqrt(np.sum((cellX_coords-cell_com)**2, axis=1)), axis=1\n",
    "    )\n",
    "\n",
    "    # Offset DM positions by smoothening length of Camila's simulations.\n",
    "    eps = smooth_l / 2.\n",
    "\n",
    "    # Long-range gravity component for each cell (without including itself).\n",
    "    quot = (cellX_coords-cell_com)/np.power((com_dis**2 + eps**2), 3./2.)\n",
    "    DM_count_sync = np.expand_dims(DM_count, axis=1)\n",
    "    del com_dis\n",
    "\n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative = -G*DM_sim_mass*np.sum(DM_count_sync*quot, axis=0)\n",
    "    \n",
    "    # note: Memory peaks here, due to these arrays:\n",
    "    # print(quot.shape, DM_count_sync.shape, derivative.shape)\n",
    "    # mem_inc = gso(quot)+gso(DM_count_sync)+gso(derivative)\n",
    "    # print(mem_inc/1e6)\n",
    "    del quot, DM_count_sync\n",
    "\n",
    "    np.save(f'{out_dir}/cell{c_id}_batch{b_id}_long_range.npy', derivative)\n",
    "\n",
    "\n",
    "def chunksize_long_range(cells, core_mem_MB):\n",
    "    \n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_long_range.\n",
    "    # -> Peak memory after calculation of derivative.\n",
    "\n",
    "    elem = 8                          # 8 bytes for standard np.float64\n",
    "    mem_type1 = 3*elem                # for derivative\n",
    "    mem_type2 = cells*3*elem          # for quot\n",
    "    mem_type3 = cells*elem            # for DM_count_sync\n",
    "\n",
    "    mem_MB = (mem_type1+mem_type2+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "\n",
    "\n",
    "def chunksize_short_range(cells, DM_tot, max_DM_lim, core_mem_MB):\n",
    "\n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_short_range.\n",
    "    # -> Peak memory after calculation of ind_2D,ind_3D,etc. sorting arrays.\n",
    "\n",
    "    elem = 8                               # 8 bytes for standard np.float64\n",
    "    mem_type0 = cells*3 * elem             # for list to ndarray of cell_coords\n",
    "    mem_type1 = cells*DM_tot * elem        # for ind_2D\n",
    "    mem_type2 = cells*DM_tot*3 * elem      # for DM_pos_sync, ind_3D, DM_sort\n",
    "    mem_type3 = cells*max_DM_lim*3 * elem  # for DM_in\n",
    "\n",
    "    mem_MB = (mem_type0+mem_type1+(3*mem_type2)+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "\n",
    "\n",
    "def batch_generators_short_range(cell_coords, cell_gen, chunksize):\n",
    "\n",
    "    cells = len(cell_coords)\n",
    "\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "    print(f'Nr. of batches (short-range): {batches}')\n",
    "    batch_arr = np.arange(batches)\n",
    "\n",
    "    cell_chunks = chunks(chunksize, cell_coords)\n",
    "    cgen_chunks = chunks(chunksize, cell_gen)\n",
    "    \n",
    "    return batch_arr, cell_chunks, cgen_chunks\n",
    "\n",
    "\n",
    "def batch_generators_long_range(\n",
    "    cell_coords, com_coords, DM_counts,\n",
    "    chunksize \n",
    "):\n",
    "    cells = len(cell_coords)\n",
    "    cell_nums = np.arange(cells)\n",
    "\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "    print(f'Nr. of batches (long-range): {batches}')\n",
    "\n",
    "    # Arrays used for naming files.\n",
    "    id_arr = np.array([idx+1 for idx in cell_nums for _ in range(batches)])\n",
    "    batch_arr = np.array([b+1 for _ in cell_nums for b in range(batches)])\n",
    "\n",
    "    # Coord of cell, for which long-range gravity gets calculated.\n",
    "    coord_arr = np.array([cc for cc in cell_coords for _ in range(batches)])\n",
    "\n",
    "    # Chunks for DM_count array, as a generator for all cells.\n",
    "    count_gens = (c for _ in cell_nums for c in chunks(chunksize, DM_counts))\n",
    "    count_chain = chain(gen for gen in count_gens)\n",
    "\n",
    "    # Chunks for cell_com array, as a generator for all cells.\n",
    "    com_gens = (c for _ in cell_nums for c in chunks(chunksize, com_coords))\n",
    "    com_chain = chain(gen for gen in com_gens)\n",
    "\n",
    "    return id_arr, batch_arr, coord_arr, count_chain, com_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from cell division.\n",
    "fin_grid = np.load(f'{TEMP_DIR}/fin_grid_{IDname}.npy')\n",
    "cell_gen = np.load(f'{TEMP_DIR}/cell_gen_{IDname}.npy')\n",
    "print(fin_grid.shape)\n",
    "\n",
    "cell_coords = np.squeeze(fin_grid, axis=1)\n",
    "\n",
    "print('cell and gen shape:', cell_coords.shape, cell_gen.shape)\n",
    "cells = len(cell_coords)\n",
    "\n",
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = 224*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / psutil.cpu_count()\n",
    "print('Per core mem:', core_mem_MB)\n",
    "\n",
    "# Determine short-range chuncksize based on available memory and cells.\n",
    "chunksize_sr = chunksize_short_range(\n",
    "    cells, len(DM_raw), PRE.DM_LIM*SHELL_MULTIPLIERS[-1], core_mem_MB\n",
    ")\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "batch_arr, cell_chunks, cgen_chunks = batch_generators_short_range(\n",
    "    cell_coords, cell_gen, chunksize_sr\n",
    ")\n",
    "\n",
    "\n",
    "# Memory profile the short-range gravity function.\n",
    "# in1 = next(cell_chunks)\n",
    "# in2 = next(cgen_chunks)\n",
    "# in3 = batch_arr[0]\n",
    "\n",
    "# cell_gravity_short_range(\n",
    "#     in1, in2, snap_GRID_L,\n",
    "#     DM_pos, PRE.DM_LIM, PRE.DM_SIM_MASS, PRE.SMOOTH_L, \n",
    "#     TEMP_DIR, in3\n",
    "# )\n",
    "\n",
    "with ProcessPoolExecutor(128) as ex:\n",
    "    ex.map(\n",
    "        cell_gravity_short_range, \n",
    "        cell_chunks, cgen_chunks, repeat(snap_GRID_L), repeat(DM_pos), \n",
    "        repeat(PRE.DM_LIM), repeat(PRE.DM_SIM_MASS), repeat(PRE.SMOOTH_L), \n",
    "        repeat(TEMP_DIR), batch_arr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dPsi_short_range_batches = [\n",
    "    np.load(f'{TEMP_DIR}/batch{b}_short_range.npy') for b in batch_arr\n",
    "]\n",
    "dPsi_short_range = np.array(\n",
    "    list(chain.from_iterable(dPsi_short_range_batches))\n",
    ")\n",
    "np.save(\n",
    "    f'{TEMP_DIR}/dPsi_short_range_{IDname}.npy', \n",
    "    dPsi_short_range\n",
    ")\n",
    "\n",
    "print(dPsi_short_range.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from cell division.\n",
    "fin_grid = np.load(f'{TEMP_DIR}/fin_grid_{IDname}.npy')\n",
    "DM_count = np.load(f'{TEMP_DIR}/DM_count_{IDname}.npy')\n",
    "cell_com = np.load(f'{TEMP_DIR}/cell_com_{IDname}.npy')\n",
    "\n",
    "new_grid = np.squeeze(fin_grid, axis=1)\n",
    "\n",
    "print('SHAPES:', new_grid.shape, DM_count.shape, cell_com.shape)\n",
    "cells = len(new_grid)\n",
    "\n",
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = 224*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / psutil.cpu_count()\n",
    "\n",
    "# Determine long-range chuncksize based on available memory and cells.\n",
    "chunksize_lr = chunksize_long_range(cells, core_mem_MB)\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "ids, batches, coords, count_chain, com_chain = batch_generators_long_range(\n",
    "    new_grid, cell_com, DM_count, chunksize_lr\n",
    ")\n",
    "\n",
    "\n",
    "# Memory profile the long-range gravity function.\n",
    "# in1 = ids[0]\n",
    "# in2 = batches[0]\n",
    "# in3 = coords[0]\n",
    "# in4 = next(count_chain)\n",
    "# in4 = list(count_chain)[0]\n",
    "# in5 = next(com_chain)\n",
    "# in5 = list(com_chain)[0]\n",
    "\n",
    "# cell_gravity_long_range(\n",
    "#     in1,in2,in3,in4,in5,PRE.DM_SIM_MASS,PRE.SMOOTH_L,TEMP_DIR\n",
    "# )\n",
    "\n",
    "# Pass arrays and chains of generators to multiprocessing routine.\n",
    "with ProcessPoolExecutor(128) as ex:\n",
    "    ex.map(\n",
    "        cell_gravity_long_range, ids, batches, \n",
    "        coords, count_chain, com_chain,\n",
    "        repeat(PRE.DM_SIM_MASS), repeat(PRE.SMOOTH_L), repeat(TEMP_DIR)\n",
    "    )\n",
    "\n",
    "# with Pool(128) as pool:\n",
    "#     pool.starmap(\n",
    "#         cell_gravity_long_range, zip(\n",
    "#         ids, batches, \n",
    "#         coords, count_chain, com_chain,\n",
    "#         repeat(PRE.DM_SIM_MASS), repeat(PRE.SMOOTH_L), repeat(TEMP_DIR))\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dPsi(c_id, batches, out_dir):\n",
    "    \n",
    "    # Load all batches for current cell.\n",
    "    dPsi_raw = np.array(\n",
    "        [np.load(f'{out_dir}/cell{c_id}_batch{b}_long_range.npy') for b in batches]\n",
    "    )\n",
    "\n",
    "    # Delete entry, which corresponds to current cell. That \"in-cell\" gravity \n",
    "    # is calculated more precisely with the short-range gravity function.\n",
    "    dPsi_cell_i = np.sum(np.delete(dPsi_raw, c_id-1, axis=0), axis=0)\n",
    "    np.save(f'{out_dir}/cell{c_id}_long_range.npy', dPsi_cell_i)    \n",
    "\n",
    "\n",
    "# Pass arrays and chains of generators to multiprocessing routine.\n",
    "with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "    ex.map(load_dPsi, ids, repeat(batches))\n",
    "\n",
    "# Save long-range gravity (a [x,y,z] array) for all cells.\n",
    "dPsi_cells = [np.load(f'{TEMP_DIR}/cell{c}_long_range.npy') for c in ids]\n",
    "np.save(f'{TEMP_DIR}/dPsi_grid_{IDname}.npy', np.array(dPsi_cells))\n",
    "\n",
    "print(np.array(dPsi_cells).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
