{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.60946484375\n",
      "********************* Initialization *********************\n",
      "# Initial conditions for neutrinos:\n",
      "PHIs = 10, THETAs=10, Vs=100\n",
      "Total neutrinos: 10000\n",
      "# Simulation parameters:\n",
      "Simulation box: L025N752\n",
      "Snapshot from 0036 (z=0) to 0013 (z=4)\n",
      "Pre/Sim CPUs 128/128\n",
      "DM limit for cells: 20000\n",
      "# File management:\n",
      "Box files directory: \n",
      " /projects/0/einf180/Tango_sims/L025N752/DMONLY/SigmaConstant00\n",
      "Output directory: \n",
      " /gpfs/home4/zimmer/neutrino_clustering_V2/L025N752/DMONLY/SigmaConstant00\n",
      "**********************************************************\n",
      "********Number density band********\n",
      "Halo batch params (Rvir,Mvir,cNFW):\n",
      "[[253.5523526   12.24058042   6.9286026 ]]\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "# See how much memory is used by OS initially.\n",
    "# Then substract this value from later mem used, to obtain mem used by scripts.\n",
    "import psutil\n",
    "GB_UNIT = 1000*1024**2\n",
    "MB_UNIT = GB_UNIT/1e3\n",
    "OS_MEM = psutil.virtual_memory().used\n",
    "print(OS_MEM/GB_UNIT)\n",
    "\n",
    "\n",
    "from shared.preface import *\n",
    "import shared.functions as fct\n",
    "\n",
    "\n",
    "# Initialize parameters and files.\n",
    "PRE = PRE(\n",
    "    sim='L025N752', \n",
    "    z0_snap=36, z4_snap=13, DM_lim=20000,\n",
    "    sim_dir=SIM_ROOT, sim_ver=SIM_TYPE,\n",
    "    phis=10, thetas=10, vels=100,\n",
    "    pre_CPUs=128, sim_CPUs=128, mem_lim_GB=224\n",
    ")\n",
    "\n",
    "\n",
    "TEMP_DIR = f'X_tests'\n",
    "\n",
    "mass_gauge = 12.0\n",
    "mass_range = 0.6\n",
    "size = 1\n",
    "\n",
    "hname = f'1e+{mass_gauge}_pm{mass_range}Msun'\n",
    "fct.halo_batch_indices(\n",
    "    PRE.Z0_STR, mass_gauge, mass_range, 'halos', size, \n",
    "    hname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "halo_batch_IDs = np.load(f'{TEMP_DIR}/halo_batch_{hname}_indices.npy')\n",
    "halo_batch_params = np.load(f'{TEMP_DIR}/halo_batch_{hname}_params.npy')\n",
    "halo_num = len(halo_batch_params)\n",
    "\n",
    "print('********Number density band********')\n",
    "print('Halo batch params (Rvir,Mvir,cNFW):')\n",
    "print(halo_batch_params)\n",
    "print('***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo 1/1 ; snapshot 0036\n",
      "(176, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# =============================================== #\n",
    "# Run precalculations for selected halo in batch. #\n",
    "# =============================================== #\n",
    "\n",
    "halo_j = 0\n",
    "halo_ID = halo_batch_IDs[halo_j]\n",
    "\n",
    "halo_rvir = halo_batch_params[halo_j, 0]\n",
    "halo_Mvir = halo_batch_params[halo_j, 1]\n",
    "halo_cNFW = halo_batch_params[halo_j, 2]\n",
    "\n",
    "# Generate progenitor index array for current halo.\n",
    "splits = re.split('/', SIM_TYPE)\n",
    "MTname = f'{PRE.SIM}_{splits[0]}_{splits[1]}'\n",
    "proj_IDs = fct.read_MergerTree(PRE.OUT_DIR, MTname, halo_ID)\n",
    "\n",
    "\n",
    "j = 0\n",
    "snap = PRE.NUMS_SNAPS[::-1][0]\n",
    "proj_ID = proj_IDs[0]\n",
    "\n",
    "proj_ID = int(proj_ID)\n",
    "\n",
    "# Output halo progress.\n",
    "print(f'halo {halo_j+1}/{halo_num} ; snapshot {snap}')\n",
    "\n",
    "# --------------------------- #\n",
    "# Read and load DM positions. #\n",
    "# --------------------------- #\n",
    "\n",
    "IDname = f'origID{halo_ID}_snap_{snap}'\n",
    "fct.read_DM_halo_index(\n",
    "    snap, proj_ID, IDname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "DM_raw = np.load(f'{TEMP_DIR}/DM_pos_{IDname}.npy')\n",
    "DM_particles = len(DM_raw)\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "# Cell division process. #\n",
    "# ---------------------- #\n",
    "\n",
    "# Initialize grid.\n",
    "snap_GRID_L = (int(np.abs(DM_raw).max()) + 1)*kpc\n",
    "raw_grid = fct.grid_3D(snap_GRID_L, snap_GRID_L)\n",
    "init_grid = np.expand_dims(raw_grid, axis=1)\n",
    "\n",
    "# Prepare arrays for cell division.\n",
    "DM_raw *= kpc\n",
    "DM_pos = np.expand_dims(DM_raw, axis=0)\n",
    "DM_pos_for_cell_division = np.repeat(DM_pos, len(init_grid), axis=0)\n",
    "del DM_raw\n",
    "\n",
    "# Cell division.\n",
    "cell_division_count = fct.cell_division(\n",
    "    init_grid, DM_pos_for_cell_division, snap_GRID_L, PRE.DM_LIM, None, TEMP_DIR, IDname\n",
    ")\n",
    "del DM_pos_for_cell_division\n",
    "\n",
    "\n",
    "# Load files from cell division.\n",
    "fin_grid = np.load(f'{TEMP_DIR}/fin_grid_{IDname}.npy')\n",
    "DM_count = np.load(f'{TEMP_DIR}/DM_count_{IDname}.npy')\n",
    "cell_com = np.load(f'{TEMP_DIR}/cell_com_{IDname}.npy')\n",
    "cell_gen = np.load(f'{TEMP_DIR}/cell_gen_{IDname}.npy')\n",
    "print(fin_grid.shape)\n",
    "\n",
    "# --------------------------------------------- #\n",
    "# Calculate gravity grid (in batches of cells). #\n",
    "# --------------------------------------------- #\n",
    "cell_coords = np.squeeze(fin_grid, axis=1)\n",
    "cells = len(cell_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell shows the reason, why beyond the virial radius, the ratio of the cell gravity and NFW has a different value than one. NFW uses the virial mass, whereas my implementation uses the total mass of all DM particles of the halo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = 10**halo_Mvir\n",
    "mass2 = PRE.DM_SIM_MASS*DM_particles/Msun\n",
    "print(mass2/mass1)\n",
    "# The last ratio is exactly what we can see (outside the virial radius) with \n",
    "# the gravity_comparison_NFW function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per core mem (GB): 1.748969024658203\n",
      "[7.85078845e-37 8.62412588e-37 7.53311526e-37] 1.3883740562482293e-36\n"
     ]
    }
   ],
   "source": [
    "def chunksize_short_range(cells, DM_tot, max_DM_lim, core_mem_MB):\n",
    "\n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_short_range.\n",
    "    # -> Peak memory after calculation of ind_2D,ind_3D,etc. sorting arrays.\n",
    "\n",
    "    elem = 8                               # 8 bytes for standard np.float64\n",
    "    mem_type0 = cells*3 * elem             # for list to ndarray of cell_coords\n",
    "    mem_type1 = cells*DM_tot * elem        # for ind_2D\n",
    "    mem_type2 = cells*DM_tot*3 * elem      # for DM_pos_sync, ind_3D, DM_sort\n",
    "    mem_type3 = cells*max_DM_lim*3 * elem  # for DM_in\n",
    "\n",
    "    mem_MB = (mem_type0+mem_type1+(3*mem_type2)+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "\n",
    "\n",
    "def batch_generators_short_range(cell_coords, cell_gen, chunksize):\n",
    "\n",
    "    cells = len(cell_coords)\n",
    "\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "    batch_arr = np.arange(batches)\n",
    "\n",
    "    cell_chunks = list(chunks(chunksize, cell_coords))\n",
    "    cgen_chunks = list(chunks(chunksize, cell_gen))\n",
    "    \n",
    "    return batch_arr, cell_chunks, cgen_chunks\n",
    "\n",
    "\n",
    "def cell_gravity_short_range(\n",
    "    cell_coords_in, cell_gen, init_GRID_S,\n",
    "    DM_pos, DM_lim, DM_sim_mass, smooth_l,\n",
    "    out_dir, b_id, max_b_len\n",
    "):\n",
    "\n",
    "    cell_coords = np.expand_dims(np.array(cell_coords_in), axis=1)\n",
    "    cell_gen = np.array(cell_gen)\n",
    "\n",
    "    # Center all DM positions w.r.t. cell center.\n",
    "    # DM_pos already in shape = (1, DM_particles, 3)\n",
    "    DM_pos_sync = np.repeat(DM_pos, len(cell_coords), axis=0)\n",
    "    DM_pos_sync -= cell_coords\n",
    "\n",
    "    # Cell lengths to limit DM particles. Limit for the largest cell is \n",
    "    # GRID_S/2, not just GRID_S, therefore the cell_gen+1 !\n",
    "    cell_len = np.expand_dims(init_GRID_S/(2**(cell_gen+1)), axis=1)\n",
    "\n",
    "    # Select DM particles inside each cell based on cube length generation.\n",
    "    DM_in_cell_IDs = np.asarray(\n",
    "        (np.abs(DM_pos_sync[:,:,0]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,1]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,2]) < cell_len)\n",
    "    )\n",
    "    #? < results in 1 missing DM particle. Using <= though overcounts\n",
    "    #? is there a way to get every DM particle by adjusting rtol and atol ?\n",
    "    # -> not pressing for now however\n",
    "    del cell_gen, cell_len\n",
    "\n",
    "    # Set DM outside cell to nan values.\n",
    "    DM_pos_sync[~DM_in_cell_IDs] = np.nan\n",
    "\n",
    "    # Save the DM IDs, such that we know which particles are in which cell.\n",
    "    # This will be used in the long-range gravity calculations.\n",
    "    DM_in_cell_IDs_compact = np.argwhere(DM_in_cell_IDs==True)\n",
    "    DM_in_cell_IDs_compact[:,0] += (max_b_len*b_id)\n",
    "\n",
    "    del DM_in_cell_IDs\n",
    "    np.save(f'{out_dir}/batch{b_id}_DM_in_cell_IDs.npy', DM_in_cell_IDs_compact)\n",
    "    del DM_in_cell_IDs_compact\n",
    "\n",
    "    # Sort all nan values to the bottom of axis 1, i.e. the DM-in-cell-X axis \n",
    "    # and truncate array based on DM_lim parameter. This simple way works since \n",
    "    # each cell cannot have more than DM_lim.\n",
    "    ind_2D = DM_pos_sync[:,:,0].argsort(axis=1)\n",
    "    ind_3D = np.repeat(np.expand_dims(ind_2D, axis=2), 3, axis=2)\n",
    "    DM_sort = np.take_along_axis(DM_pos_sync, ind_3D, axis=1)\n",
    "    DM_in = DM_sort[:,:DM_lim*SHELL_MULTIPLIERS[-1],:]\n",
    "\n",
    "    # note: Memory peaks here, due to these arrays:\n",
    "    # print(DM_pos_sync.shape, ind_2D.shape, ind_3D.shape, DM_sort.shape, DM_in.shape)\n",
    "    # mem_inc = gso(cell_coords)+gso(DM_pos_sync)+gso(ind_2D)+gso(ind_3D)+gso(DM_sort)+gso(DM_in)\n",
    "    # print('MEM_PEAK:', mem_inc/1e6)\n",
    "    del DM_pos_sync, ind_2D, ind_3D, DM_sort\n",
    "\n",
    "    # Calculate distances of DM and adjust array dimensionally.\n",
    "    DM_dis = np.expand_dims(np.sqrt(np.sum(DM_in**2, axis=2)), axis=2)\n",
    "\n",
    "    # Offset DM positions by smoothening length of Camila's simulations.\n",
    "    eps = smooth_l / 2.\n",
    "\n",
    "    # Quotient in sum (see formula). Can contain nan values, thus the np.nansum for the derivative, s.t. these values don't contribute.\n",
    "    quot = (cell_coords - DM_in)/np.power((DM_dis**2 + eps**2), 3./2.)\n",
    "    \n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative = -G*DM_sim_mass*np.nansum(quot, axis=1)    \n",
    "    np.save(f'{out_dir}/batch{b_id}_short_range.npy', derivative)\n",
    "\n",
    "\n",
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = PRE.MEM_LIM_GB*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / PRE.PRE_CPUs\n",
    "print('Per core mem (GB):', core_mem_MB/1e3)\n",
    "\n",
    "# Determine short-range chuncksize based on available memory and cells.\n",
    "chunksize_sr = chunksize_short_range(\n",
    "    cells, DM_particles, PRE.DM_LIM*SHELL_MULTIPLIERS[-1], core_mem_MB\n",
    ")\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "batch_arr, cell_chunks, cgen_chunks = batch_generators_short_range(\n",
    "    cell_coords, cell_gen, chunksize_sr\n",
    ")\n",
    "\n",
    "single = False\n",
    "\n",
    "if single:\n",
    "    # Memory profile the short-range gravity function.\n",
    "    cell_index = -1\n",
    "    in1 = cell_chunks[cell_index]\n",
    "    in2 = cgen_chunks[cell_index]\n",
    "    in3 = batch_arr[cell_index]\n",
    "\n",
    "    cell_gravity_short_range(\n",
    "        in1, in2, snap_GRID_L,\n",
    "        DM_pos, PRE.DM_LIM, PRE.DM_SIM_MASS, PRE.SMOOTH_L, \n",
    "        TEMP_DIR, in3, chunksize_sr\n",
    "    )\n",
    "else:\n",
    "    with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "        ex.map(\n",
    "            cell_gravity_short_range, \n",
    "            cell_chunks, cgen_chunks, repeat(snap_GRID_L), repeat(DM_pos), \n",
    "            repeat(PRE.DM_LIM), repeat(PRE.DM_SIM_MASS), repeat(PRE.SMOOTH_L), \n",
    "            repeat(TEMP_DIR), batch_arr, repeat(chunksize_sr)\n",
    "        )\n",
    "\n",
    "    # Combine short-range batch files.\n",
    "    dPsi_short_range_batches = [\n",
    "        np.load(f'{TEMP_DIR}/batch{b}_short_range.npy') for b in batch_arr\n",
    "    ]\n",
    "    dPsi_short_range = np.array(\n",
    "        list(chain.from_iterable(dPsi_short_range_batches))\n",
    "    )\n",
    "    np.save(\n",
    "        f'{TEMP_DIR}/dPsi_short_range_{IDname}.npy', \n",
    "        dPsi_short_range\n",
    "    )\n",
    "    tx = 0\n",
    "    print(dPsi_short_range[tx], np.sqrt(np.sum(dPsi_short_range[tx]**2)))\n",
    "\n",
    "    # Combine DM_in_cell_IDs batches.\n",
    "    DM_in_cell_IDs_l = []\n",
    "    for b_id in batch_arr:\n",
    "        DM_in_cell_IDs_l.append(\n",
    "            np.load(f'{TEMP_DIR}/batch{b_id}_DM_in_cell_IDs.npy')\n",
    "        )\n",
    "    DM_in_cell_IDs_np = np.array(list(chain.from_iterable(DM_in_cell_IDs_l)))\n",
    "    np.save(\n",
    "        f'{TEMP_DIR}/DM_in_cell_IDs_{IDname}.npy', \n",
    "        DM_in_cell_IDs_np\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunksize_long_range(cells, core_mem_MB):\n",
    "    \n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_long_range.\n",
    "    # -> Peak memory after calculation of derivative.\n",
    "\n",
    "    elem = 8                          # 8 bytes for standard np.float64\n",
    "    mem_type1 = 3*elem                # for derivative\n",
    "    mem_type2 = cells*3*elem          # for quot\n",
    "    mem_type3 = cells*elem            # for DM_count_sync\n",
    "\n",
    "    mem_MB = (mem_type1+mem_type2+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "    \n",
    "\n",
    "def batch_generators_long_range(\n",
    "    cell_coords, com_coords, cell_gen, \n",
    "    DM_counts, chunksize \n",
    "):\n",
    "\n",
    "    # Cell number, chunksize and resulting number of batches.\n",
    "    cells = len(cell_coords)\n",
    "    cell_nums = np.arange(cells)\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "\n",
    "\n",
    "    ### ------- ###\n",
    "    ### Arrays. ###\n",
    "    ### ------- ###\n",
    "\n",
    "    # Arrays are repeated/tiled to match length & number of batches.\n",
    "\n",
    "    batch_IDs = np.tile(np.arange(batches), cells)  # batch labels/indices\n",
    "    # [0,...,batches,0,...,batches,...]\n",
    "    \n",
    "    cellC_rep = np.repeat(cell_nums, batches)  # cell labels/indices\n",
    "    # [0,...,0,1,...,1,...,cells-1,...,cells-1]\n",
    "\n",
    "    cellC_cc = np.repeat(cell_coords, batches, axis=0)  # cell coordinates\n",
    "    # [[x1,y1,z1],...,[x1,y1,z1],...]\n",
    "\n",
    "    cellC_gen = np.repeat(cell_gen, batches)  # cell generations\n",
    "    # e.g. [0,...,0,0,...,0,1,...1,...], ... repeated to match nr of batches\n",
    "\n",
    "    ### ----------- ###\n",
    "    ### Generators. ###\n",
    "    ### ----------- ###\n",
    "\n",
    "    # We split up all arrays into smaller chunks, s.t. each batch gets a \n",
    "    # smaller set of cells. Generators are used to benefit from the chunks() \n",
    "    # function, then converted to lists (map() will do this anyway).\n",
    "    \n",
    "    # We do this for:\n",
    "    # 1. the cell_IDs of cells present in a batch, \n",
    "    # 2. the DM counts of all cells in a batch, \n",
    "    # 3. the c.o.m. coords of all cells in a batch,\n",
    "    # 4. the (cell division) generation index for all cells in a batch:\n",
    "\n",
    "    # 1. \n",
    "    cib_IDs = list(\n",
    "        (c for _ in cell_nums for c in chunks(chunksize, cell_nums))\n",
    "    )\n",
    "\n",
    "    # 2.\n",
    "    counts = list(\n",
    "        (c for _ in cell_nums for c in chunks(chunksize, DM_counts))\n",
    "    )\n",
    "\n",
    "    # 3.\n",
    "    coms = list(\n",
    "        (c for _ in cell_nums for c in chunks(chunksize, com_coords))\n",
    "    )\n",
    "\n",
    "    # 4.\n",
    "    gens = list(\n",
    "        (c for _ in cell_nums for c in chunks(chunksize, cell_gen))\n",
    "    )\n",
    "\n",
    "    # Previously, I did the following additonally, which led to overcounting?\n",
    "    # gen_chain = chain(gen for gen in gen_gens)\n",
    "\n",
    "    return batch_IDs, cellC_rep, cellC_cc, cellC_gen, \\\n",
    "        cib_IDs, counts, coms, gens\n",
    "\n",
    "\n",
    "def load_dPsi_long_range(c_id, batches, out_dir):\n",
    "\n",
    "    # Load all batches for current cell.\n",
    "    dPsi_raw = np.array(\n",
    "        [np.load(f'{out_dir}/cell{c_id}_batch{b}_long_range.npy') for b in batches]\n",
    "    )\n",
    "\n",
    "    # Combine into one array by summing and save.\n",
    "    dPsi_for_cell = np.sum(dPsi_raw, axis=0)\n",
    "    np.save(f'{out_dir}/cell{c_id}_long_range.npy', dPsi_for_cell)  \n",
    "\n",
    "\n",
    "def cell_gravity_long_range_quadrupole(\n",
    "    c_id, cib_ids, b_id, cellC_cc, cell_com, cell_gen, init_GRID_S,\n",
    "    DM_pos, DM_count, DM_in_cell_IDs, DM_sim_mass, out_dir, \n",
    "    max_b_len, cellC_gen\n",
    "):\n",
    "    # Prefix \"cellC\" denotes the cell to calculate the long-range forces for.\n",
    "\n",
    "    # Convert the list inputs to numpy arrays.\n",
    "    cib_ids = np.array(cib_ids)\n",
    "    cell_com = np.array(cell_com)\n",
    "    cell_gen = np.array(cell_gen)\n",
    "    DM_count = np.array(DM_count)\n",
    "\n",
    "    # Array, where cell C is centered on c.o.m. coords. of all cells.\n",
    "    cellC_sync = np.repeat(\n",
    "        np.expand_dims(cellC_cc, axis=0), len(cell_com), axis=0\n",
    "    ) \n",
    "    cellC_sync -= cell_com\n",
    "\n",
    "    # Get (complete, not half) cell length of all cells and current cell C.\n",
    "    cell_len = init_GRID_S/(2**(cell_gen))\n",
    "    cellC_len = init_GRID_S/(2**(cellC_gen))\n",
    "\n",
    "    # Distance of cellC to all cell_com's.\n",
    "    cellC_dis = np.sqrt(np.sum(cellC_sync**2, axis=1))\n",
    "\n",
    "\n",
    "    if c_id in cib_ids:\n",
    "        # Overwrite element corresponding to cell C from all arrays, to avoid\n",
    "        # self-gravity of the cell (taken care of by short-range gravity).\n",
    "\n",
    "        # First find the correct index for the current cell. We have to take \n",
    "        # this akin to a \"modulo\" according to the maximum batch length. E.g. \n",
    "        # if the max. batch length is 100, but we are at cell label/index 100, \n",
    "        # then this would be index '0' again for arrays in this function.\n",
    "        cellC_idx = (c_id - (max_b_len*b_id))\n",
    "\n",
    "        # Set cell C array element(s) to 0 or nan, s.t. it just adds 0 later on.\n",
    "        cell_com[cellC_idx] = np.nan  # c.o.m. of cell C to [nan,nan,nan]\n",
    "        cell_len[cellC_idx] = 0  # 0 cell length\n",
    "        DM_count[cellC_idx] = 0  # 0 DM count\n",
    "\n",
    "        # If cell C has 0 DM, then its c.o.m == coords., so set cellC_dis to 1.\n",
    "        # (avoids divide by zero in these (edge) cases later on)\n",
    "        if cellC_dis[cellC_idx] == 0:\n",
    "            cellC_dis[cellC_idx] = 1\n",
    "\n",
    "\n",
    "    # Determine which cells are close enough and need multipole expansion:    \n",
    "    # Cells with angle larger than the critical angle are multipole cells.\n",
    "    theta = cell_len / cellC_dis\n",
    "\n",
    "    # The critical angle is dependent on the cell length of cell C.\n",
    "    # (see GoodNotes for why 0.3 exponent for now)\n",
    "    theta_crit = 1/(1.5*((cellC_gen+1)**0.3))\n",
    "\n",
    "    # Divide all cell IDs into multipole cells and monopole-only cells.\n",
    "    multipole_IDs = np.argwhere(theta >= theta_crit).flatten()\n",
    "    monopole_IDs = np.argwhere(theta < theta_crit).flatten()\n",
    "    # note: cell C will be a monopole-only cell (we set cell length to 0), but \n",
    "    # note: with DM_count set to 0.\n",
    "\n",
    "    # DM count for multipole cells.\n",
    "    DM_count_mpoles = DM_count[multipole_IDs]\n",
    "\n",
    "    # All DM particles (their positions), which are in multipole cells.\n",
    "    # Adjust/match multipole_IDs first, since they are limited to size of batch.\n",
    "    comparison_IDs = multipole_IDs + (max_b_len*b_id)\n",
    "    DM_IDs = DM_in_cell_IDs[np.in1d(DM_in_cell_IDs[:,0], comparison_IDs)][:,1]\n",
    "    DM_pos_mpoles = DM_pos[np.in1d(np.arange(len(DM_pos)), DM_IDs)]\n",
    "\n",
    "    # Split this total DM array into DM chunks present in each multipole cell:\n",
    "    # We can do this by breaking each axis into sub-arrays, with length \n",
    "    # depending on how many DM particles are in each cell (which is stored in \n",
    "    # DM_count of multipole cells, i.e. DM_count_mpoles).\n",
    "\n",
    "    # Special case, where all multipole cells have no DM.\n",
    "    # (This can happen e.g. if cell C is an outermost cell)\n",
    "    if np.all(DM_count_mpoles==0):\n",
    "        DM_mpoles = np.full(shape=(len(multipole_IDs),1,3), fill_value=np.nan)\n",
    "\n",
    "    # \"Normal\" case, where some or all multipole cells contain DM.\n",
    "    else:\n",
    "        breaks = np.cumsum(DM_count_mpoles[:-1])\n",
    "        ax0_split = np.split(DM_pos_mpoles[:,0], breaks)\n",
    "        ax1_split = np.split(DM_pos_mpoles[:,1], breaks)\n",
    "        ax2_split = np.split(DM_pos_mpoles[:,2], breaks)\n",
    "\n",
    "        # Fill each axis with nans to obtain dimensionally valid ndarray.\n",
    "        DM_axis0 = np.array(list(zip_longest(*ax0_split, fillvalue=np.nan))).T\n",
    "        DM_axis1 = np.array(list(zip_longest(*ax1_split, fillvalue=np.nan))).T\n",
    "        DM_axis2 = np.array(list(zip_longest(*ax2_split, fillvalue=np.nan))).T\n",
    "\n",
    "        # Recombine all axes into one final DM positions array.\n",
    "        DM_mpoles = np.stack((DM_axis0, DM_axis1, DM_axis2), axis=2)\n",
    "\n",
    "    # Select c.o.m. of multipole cells and adjust dimensionally.\n",
    "    mpoles_com = np.expand_dims(cell_com[multipole_IDs], axis=1)\n",
    "\n",
    "    # Center DM in multipole (now labeled \"J\") cells on their c.o.m. coords.\n",
    "    DM_mpoles -= mpoles_com\n",
    "\n",
    "    # Calculate distances of all DM in J cells from their c.o.m. coord.\n",
    "    # -> this is the r_j needed for the quadrupole term.\n",
    "    DM_dis = np.expand_dims(np.sqrt(np.sum(DM_mpoles**2, axis=2)), axis=2)\n",
    "\n",
    "    # Array, where cell C is centered on c.o.m. coords. of J cells.\n",
    "    cellC_Jcoms = np.repeat(\n",
    "        np.expand_dims(np.expand_dims(cellC_cc, axis=0), axis=0), \n",
    "        len(mpoles_com), axis=0\n",
    "    )\n",
    "    cellC_Jcoms -= mpoles_com\n",
    "\n",
    "    # Distance of cell C to c.o.m. of multipole cells.\n",
    "    cellC_dis = np.sqrt(np.sum(cellC_Jcoms**2, axis=2))\n",
    "\n",
    "\n",
    "    ### -------------------- ###\n",
    "    ### Quadrupole formulas. ###\n",
    "    ### -------------------- ###\n",
    "    # See GoodNotes for notation.\n",
    "\n",
    "    # (x,y,z) positions of individual DM particles in multipole cells.\n",
    "    # (these are each w.r.t the c.o.m. of the corresponding multipole cell)\n",
    "    # -> notation: x_{a,j}\n",
    "    x_1j = DM_mpoles[...,0]\n",
    "    x_2j = DM_mpoles[...,1]\n",
    "    x_3j = DM_mpoles[...,2]\n",
    "\n",
    "    # (x,y,z) positions of cell C, w.r.t to the c.o.m.'s of the multipole cells.\n",
    "    # -> notation: x_a\n",
    "    x_1 = cellC_Jcoms[...,0]\n",
    "    x_2 = cellC_Jcoms[...,1]\n",
    "    x_3 = cellC_Jcoms[...,2]\n",
    "\n",
    "    # The sum in the long-range multipole potential, where a = b.\n",
    "    a_eq_b_orig = np.nansum(\n",
    "        np.nansum((3*DM_mpoles**2 - DM_dis**2)*cellC_Jcoms**2, axis=1), axis=1)\n",
    "\n",
    "    # The sum in the long-range multipole potential, where a != b.\n",
    "    a_neq_b_orig = 3*2*np.nansum(\n",
    "        x_1j*x_2j*x_1*x_2 + x_2j*x_3j*x_2*x_3 + x_3j*x_1j*x_3*x_1, axis=1)  \n",
    "\n",
    "    # Sum of the above sums.\n",
    "    # -> notation: sum_{J,a}\n",
    "    sum_Ja_orig = np.expand_dims(a_eq_b_orig + a_neq_b_orig, axis=1)\n",
    "\n",
    "    # Sums for a=b and a!=b, which appear in the long-range derivative.\n",
    "    a_eq_b_deriv = np.nansum(\n",
    "        np.nansum((3*DM_mpoles**2 - DM_dis**2)*cellC_Jcoms, axis=1), axis=1)\n",
    "    a_neq_b_deriv = 3*np.nansum(\n",
    "        x_1j*x_2j*x_2 + x_2j*x_3j*x_3 + x_3j*x_1j*x_1, axis=1)\n",
    "\n",
    "    # Sum of the above sums. Global factor of 2 due to derivative cancels with \n",
    "    # 2 in denominator (see formula), so both are ommited to save calculations.\n",
    "    sum_Ja_deriv = np.expand_dims(a_eq_b_deriv + a_neq_b_deriv, axis=1)\n",
    "\n",
    "    # Global sum over all multipole cells J!=C ; a summation over axis 0.\n",
    "    cellC_Jcoms_2D = np.squeeze(cellC_Jcoms, axis=1)\n",
    "    \n",
    "    term1 = cellC_Jcoms_2D/cellC_dis**3\n",
    "    term2 = 5*cellC_Jcoms_2D*sum_Ja_orig/(2*cellC_dis**7)\n",
    "    term3 = sum_Ja_deriv/cellC_dis**5\n",
    "\n",
    "    # Contribution of multipole cells to derivative (see master formula).\n",
    "    # np.nansum, since previous terms can have nan values, and nan-only cells \n",
    "    # do not contribute.\n",
    "    DM_count_mpoles_sync = np.expand_dims(DM_count_mpoles, axis=1)\n",
    "    dPsi_multipole_cells = G*DM_sim_mass*np.nansum(\n",
    "        DM_count_mpoles_sync*(term1+term2-term3), axis=0)\n",
    "\n",
    "\n",
    "    ### -------------------------- ###\n",
    "    ### Other monopole-only cells. ###\n",
    "    ### -------------------------- ###\n",
    "\n",
    "    # DM count for monopole cells.\n",
    "    DM_count_mono = DM_count[monopole_IDs]\n",
    "\n",
    "    # All DM particles (their positions), which are in monopole cells.\n",
    "    comparison_IDs = monopole_IDs + (max_b_len*b_id)\n",
    "    DM_IDs = DM_in_cell_IDs[np.in1d(DM_in_cell_IDs[:,0], comparison_IDs)][:,1]\n",
    "    DM_pos_mono = DM_pos[np.in1d(np.arange(len(DM_pos)), DM_IDs)]\n",
    "\n",
    "    # Splitting and combining routine. See above in multipoles for comments.\n",
    "    if np.all(DM_count_mono==0):\n",
    "        DM_mono = np.full(shape=(len(monopole_IDs),1,3), fill_value=np.nan)\n",
    "    else:\n",
    "        breaks = np.cumsum(DM_count_mono[:-1])\n",
    "        ax0_split = np.split(DM_pos_mono[:,0], breaks)\n",
    "        ax1_split = np.split(DM_pos_mono[:,1], breaks)\n",
    "        ax2_split = np.split(DM_pos_mono[:,2], breaks)\n",
    "\n",
    "        DM_axis0 = np.array(list(zip_longest(*ax0_split, fillvalue=np.nan))).T\n",
    "        DM_axis1 = np.array(list(zip_longest(*ax1_split, fillvalue=np.nan))).T\n",
    "        DM_axis2 = np.array(list(zip_longest(*ax2_split, fillvalue=np.nan))).T\n",
    "\n",
    "        DM_mono = np.stack((DM_axis0, DM_axis1, DM_axis2), axis=2)\n",
    "\n",
    "    # Select c.o.m. of monopole cells (incl. cell C) and adjust dimensionally.\n",
    "    mono_com = np.expand_dims(cell_com[monopole_IDs], axis=1)\n",
    "\n",
    "    # Center DM in monopole cells on their c.o.m. coords.\n",
    "    DM_mono -= mono_com\n",
    "\n",
    "    # Array, where cell C is centered on c.o.m. coords. of monopole cells.\n",
    "    cellC_Jcoms_mono = np.repeat(\n",
    "        np.expand_dims(np.expand_dims(cellC_cc, axis=0), axis=0), \n",
    "        len(mono_com), axis=0\n",
    "    )\n",
    "    cellC_Jcoms_mono -= mono_com\n",
    "\n",
    "    # Distance of cell C to c.o.m. of monopole cells.\n",
    "    cellC_dis_mono = np.sqrt(np.sum(cellC_Jcoms_mono**2, axis=2))\n",
    "\n",
    "    # Long-range force of all monopole cells.\n",
    "    cellC_Jcoms_mono_2D = np.squeeze(cellC_Jcoms_mono, axis=1)\n",
    "    DM_count_mono_sync = np.expand_dims(DM_count_mono, axis=1)\n",
    "    dPsi_monopole_cells = G*DM_sim_mass*np.nansum(\n",
    "        DM_count_mono_sync*cellC_Jcoms_mono_2D/(cellC_dis_mono**3), axis=0)\n",
    "\n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative_lr = -(dPsi_multipole_cells + dPsi_monopole_cells)\n",
    "\n",
    "    np.save(f'{out_dir}/cell{c_id}_batch{b_id}_long_range.npy', derivative_lr)\n",
    "\n",
    "\n",
    "\n",
    "# TODOs for checking/fixing: \n",
    "# -[x] Basically go through the whole logic step by step. Particularly:\n",
    "    # -[x] Check if self-gravity is deleted in all cases (indep. of batches)\n",
    "    # -[x] Check if all nan values are appropriate and get summed correctly\n",
    "    # -[x] The indices are matched correctly and adaptable for nr. of batches\n",
    "    # -[x] Double check the quadrupole formulas\n",
    "\n",
    "# Once it's working:\n",
    "# -[] Run number_density_band with a high DM_lim (and force multiple batches?), \n",
    "#     s.t. memory is ok.\n",
    "# -[] Memory profile the whole function. #? difficult? varying size of arrays?\n",
    "\n",
    "\n",
    "\n",
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = PRE.MEM_LIM_GB*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / PRE.PRE_CPUs\n",
    "print('Per core mem (GB):', core_mem_MB/1e3)\n",
    "\n",
    "# Determine long-range chuncksize based on available memory and cells.\n",
    "# chunksize_lr = chunksize_long_range(cells, core_mem_MB)\n",
    "chunksize_lr = 100\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "DM_in_cell_IDs = np.load(f'{TEMP_DIR}/DM_in_cell_IDs_{IDname}.npy')\n",
    "batch_IDs, cellC_rep, cellC_cc, gen_rep, cib_IDs_gens, count_gens, com_gens, gen_gens = batch_generators_long_range(\n",
    "    cell_coords, cell_com, cell_gen, DM_count, chunksize_lr\n",
    ")\n",
    "\n",
    "single = True\n",
    "\n",
    "if single:\n",
    "    # Memory profile the long-range gravity function.\n",
    "    cell_index = -1\n",
    "    c_id = cellC_rep[cell_index]\n",
    "    cib_ids = cib_IDs_gens[cell_index]\n",
    "    b_id = batch_IDs[cell_index]\n",
    "    cellC_cc = cellC_cc[cell_index]\n",
    "    count_gen = count_gens[cell_index]\n",
    "    com_gen = com_gens[cell_index]\n",
    "    gen_gen = gen_gens[cell_index]\n",
    "    cellC_gen = gen_rep[cell_index]\n",
    "\n",
    "    load_dPsi_long_range(c_id, np.unique(batch_IDs), TEMP_DIR)\n",
    "\n",
    "    cell_gravity_long_range_quadrupole(\n",
    "        c_id, cib_ids, b_id, cellC_cc, com_gen, gen_gen, snap_GRID_L,\n",
    "        np.squeeze(DM_pos, axis=0), count_gen, DM_in_cell_IDs, \n",
    "        PRE.DM_SIM_MASS, TEMP_DIR, chunksize_lr, cellC_gen\n",
    "    )\n",
    "\n",
    "    output_lr = np.load(f'{TEMP_DIR}/cell{c_id}_batch{b_id}_long_range.npy')\n",
    "    print('OUTPUT:', output_lr, np.sqrt(np.sum(output_lr**2)))\n",
    "else:\n",
    "    # Pass arrays and chains of generators to multiprocessing routine.\n",
    "    with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "        ex.map(\n",
    "            cell_gravity_long_range_quadrupole, \n",
    "            cellC_rep, cib_IDs_gens, batch_IDs, \n",
    "            cellC_cc, com_gens, gen_gens, repeat(snap_GRID_L),\n",
    "            repeat(np.squeeze(DM_pos, axis=0)), count_gens, \n",
    "            repeat(DM_in_cell_IDs), repeat(PRE.DM_SIM_MASS), repeat(TEMP_DIR),\n",
    "            repeat(chunksize_lr), gen_rep\n",
    "        )\n",
    "\n",
    "    # Combine long-range batch files.\n",
    "    cell_labels = np.unique(cellC_rep)\n",
    "    batch_labels = np.unique(batch_IDs)\n",
    "    with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "        ex.map(\n",
    "            load_dPsi_long_range, cell_labels, \n",
    "            repeat(batch_labels), repeat(TEMP_DIR)\n",
    "        )\n",
    "\n",
    "    # Save long-range gravity (a [x,y,z] array) for all cells.\n",
    "    dPsi_lr = np.array(\n",
    "        [np.load(f'{TEMP_DIR}/cell{c}_long_range.npy') for c in cell_labels])\n",
    "    np.save(f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy', dPsi_lr)\n",
    "    tx_lr = -1\n",
    "\n",
    "    print(dPsi_lr[tx_lr], np.sqrt(np.sum(dPsi_lr[tx_lr]**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multipole expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR & LR comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cell = -1\n",
    "\n",
    "### For short-range. ###\n",
    "gravity_sr = np.load(f'{TEMP_DIR}/dPsi_short_range_{IDname}.npy')\n",
    "print(f'Short range gravity grid shape:', gravity_sr.shape)\n",
    "\n",
    "mags_sr = np.sqrt(np.sum(gravity_sr**2, axis=1))\n",
    "print(mags_sr[test_cell])\n",
    "\n",
    "### For long-range. ###\n",
    "gravity_lr = np.load(f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy')\n",
    "print(f'Long range gravity grid shape:', gravity_lr.shape)\n",
    "\n",
    "mags_lr = np.sqrt(np.sum(gravity_lr**2, axis=1))\n",
    "print(mags_lr[test_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_lr_comparison(\n",
    "    cell_coords, sr_grid, lr_grid,\n",
    "    Rvir, Mvir\n",
    "    ):\n",
    "\n",
    "    # Halo parameters.\n",
    "    Rvir *= kpc\n",
    "    Mvir *= 10**Mvir*Msun\n",
    "\n",
    "    # Sort cells by distance from center (0,0,0).\n",
    "    pos_grid = cell_coords/kpc\n",
    "    grid_dist = np.sqrt(np.sum(pos_grid**2, axis=1))\n",
    "    dist_order = grid_dist.argsort()\n",
    "    grid_dist = grid_dist[dist_order]\n",
    "\n",
    "    # Order short-range and long-range magnitudes.\n",
    "    sr_mags = np.sqrt(np.sum(sr_grid**2, axis=1))\n",
    "    lr_mags = np.sqrt(np.sum(lr_grid**2, axis=1))\n",
    "    sr_mags = sr_mags[dist_order]\n",
    "    lr_mags = lr_mags[dist_order]\n",
    "\n",
    "    sr_mags[sr_mags == 0] = 1\n",
    "\n",
    "    grav_ratio = lr_mags / sr_mags\n",
    "    # grav_ratio = sr_mags / lr_mags\n",
    "    print(f'min/max: {grav_ratio.min():.2e}/{grav_ratio.max():.2e}')\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    ax.set_title(f'{Mvir/Msun:.2e} Msun halo')\n",
    "    ax.scatter(\n",
    "        grid_dist, grav_ratio, s=5, alpha=0.8, \n",
    "        label=f'cells'\n",
    "    )\n",
    "    ax.axhline(1, c='green', ls=':')\n",
    "    ax.axhline(10, c='red', ls=':')\n",
    "    ax.axhline(np.min(grav_ratio), c='red', ls=':')\n",
    "    ax.axvline(\n",
    "        Rvir/kpc, c='black', ls=':', alpha=0.5, \n",
    "        label=r'$R_{vir}$ of halo'\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('distance from center (kpc)')\n",
    "    ax.set_ylabel('long-range/short-range gravity ratio')\n",
    "\n",
    "    ax.set_ylim(1e-2,1e6)\n",
    "    ax.set_yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sr_lr_comparison(\n",
    "    cell_coords, gravity_sr, gravity_lr,\n",
    "    halo_rvir, halo_Mvir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFW comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravity_comparison_NFW(cell_coords, dPsi_grid, Rvir, Mvir, cNFW, z=0):\n",
    "\n",
    "    # NFW parameters.\n",
    "    Rvir *= kpc\n",
    "    Mvir *= 10**Mvir*Msun\n",
    "    Rs = Rvir/cNFW\n",
    "    rho0 = fct.scale_density_NFW(z, cNFW)\n",
    "\n",
    "    # Calculate NFW value at cell coords.\n",
    "    NFW_cells = np.array([\n",
    "        fct.dPsi_dxi_NFW(x_i, z, rho0, Mvir, Rvir, Rs, 'HALO')\n",
    "        for x_i in cell_coords\n",
    "    ])/(kpc/s**2)\n",
    "    NFW_mags = np.sqrt(np.sum(NFW_cells**2, axis=1))\n",
    "\n",
    "    # Magnitudes of gravity in each cell.\n",
    "    dPsi_mags = np.sqrt(np.sum(dPsi_grid**2, axis=1))/(kpc/s**2)\n",
    "\n",
    "    # Sort cells by distance from center (0,0,0).\n",
    "    pos_grid = cell_coords/kpc\n",
    "    grid_dist = np.sqrt(np.sum(pos_grid**2, axis=1))\n",
    "    dist_order = grid_dist.argsort()\n",
    "\n",
    "    grid_dist = grid_dist[dist_order]\n",
    "    dPsi_mags = dPsi_mags[dist_order]\n",
    "    NFW_mags = NFW_mags[dist_order]\n",
    "\n",
    "    grav_ratio = dPsi_mags/NFW_mags\n",
    "    print(f'dPsi/NFW: min={np.min(grav_ratio)}, max={np.max(grav_ratio)}')\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    ax.set_title(f'{Mvir/Msun:.2e} Msun halo')\n",
    "    ax.scatter(\n",
    "        grid_dist, grav_ratio, s=5, alpha=0.8, \n",
    "        label=f'cells'\n",
    "    )\n",
    "    ax.axhline(1, c='green', ls=':')\n",
    "    ax.axhline(10, c='red', ls=':')\n",
    "    ax.axhline(np.min(grav_ratio), c='red', ls=':')\n",
    "    ax.axvline(\n",
    "        Rvir/kpc, c='black', ls=':', alpha=0.5, \n",
    "        label=r'$R_{vir}$ of halo'\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('distance from center (kpc)')\n",
    "    ax.set_ylabel('sim/NFW gravity ratio')\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_comparison_NFW(\n",
    "    cell_coords, gravity_sr, \n",
    "    halo_rvir, halo_Mvir, halo_cNFW, z=0\n",
    ")\n",
    "\n",
    "\n",
    "gravity_comparison_NFW(\n",
    "    cell_coords, gravity_lr, \n",
    "    halo_rvir, halo_Mvir, halo_cNFW, z=0\n",
    ")\n",
    "\n",
    "gravity_comparison_NFW(\n",
    "    cell_coords, gravity_sr+gravity_lr, \n",
    "    halo_rvir, halo_Mvir, halo_cNFW, z=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directionality of gravity grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_arrows(\n",
    "    cell_coords, dPsi_grid,\n",
    "):\n",
    "\n",
    "    # x,y,z of cell coordinates.\n",
    "    pos_grid = cell_coords/kpc\n",
    "    xGrid = pos_grid[:,0]\n",
    "    yGrid = pos_grid[:,1]\n",
    "    zGrid = pos_grid[:,2]\n",
    "\n",
    "    # x,y,z of gravity grid.\n",
    "    xPsi = dPsi_grid[:,0]\n",
    "    yPsi = dPsi_grid[:,1]\n",
    "    zPsi = dPsi_grid[:,2]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # start, stop = 0, -1 \n",
    "    start, stop = 250, -1 \n",
    "    ax.quiver(\n",
    "        xGrid[start:stop], yGrid[start:stop], zGrid[start:stop], \n",
    "        xPsi[start:stop], yPsi[start:stop], zPsi[start:stop], \n",
    "        length=10, normalize=True\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "grid_arrows(cell_coords, gravity_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
