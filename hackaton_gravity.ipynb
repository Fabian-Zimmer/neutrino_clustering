{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS memory in GB: 14.97714453125\n",
      "********************* Initialization *********************\n",
      "# Initial conditions for neutrinos:\n",
      "PHIs = 10, THETAs=10, Vs=100\n",
      "Total neutrinos: 10000\n",
      "# Simulation parameters:\n",
      "Simulation box: L025N752\n",
      "Snapshot from 0036 (z=0) to 0013 (z=4)\n",
      "Pre/Sim CPUs 128/128\n",
      "DM limit for cells: 10000\n",
      "# File management:\n",
      "Box files directory: \n",
      " /projects/0/einf180/Tango_sims/L025N752/DMONLY/SigmaConstant00\n",
      "Output directory: \n",
      " /gpfs/home4/zimmer/neutrino_clustering_V2/L025N752/DMONLY/SigmaConstant00\n",
      "**********************************************************\n",
      "********Number density band********\n",
      "Halo batch params (Rvir,Mvir,cNFW):\n",
      "[[253.5523526   12.24058042   6.9286026 ]\n",
      " [158.62979351  11.62953177   8.03988676]\n",
      " [137.07912107  11.43929142   9.41292697]]\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "# See how much memory is used by OS initially.\n",
    "# Then substract this value from later mem used, to obtain mem used by scripts.\n",
    "import psutil\n",
    "GB_UNIT = 1000*1024**2\n",
    "MB_UNIT = GB_UNIT/1e3\n",
    "OS_MEM = psutil.virtual_memory().used\n",
    "print('OS memory in GB:', OS_MEM/GB_UNIT)\n",
    "\n",
    "\n",
    "from shared.preface import *\n",
    "import shared.functions as fct\n",
    "\n",
    "\n",
    "# Initialize parameters and files.\n",
    "PRE = PRE(\n",
    "    sim='L025N752', \n",
    "    z0_snap=36, z4_snap=13, DM_lim=10000,\n",
    "    sim_dir=SIM_ROOT, sim_ver=SIM_TYPE,\n",
    "    phis=10, thetas=10, vels=100,\n",
    "    pre_CPUs=128, sim_CPUs=128, mem_lim_GB=224\n",
    ")\n",
    "\n",
    "\n",
    "TEMP_DIR = f'X_tests'\n",
    "\n",
    "# Halo parameters.\n",
    "mass_gauge = 12.0\n",
    "mass_range = 0.6\n",
    "size = 3\n",
    "\n",
    "hname = f'1e+{mass_gauge}_pm{mass_range}Msun'\n",
    "fct.halo_batch_indices(\n",
    "    PRE.Z0_STR, mass_gauge, mass_range, 'halos', size, \n",
    "    hname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "halo_batch_IDs = np.load(f'{TEMP_DIR}/halo_batch_{hname}_indices.npy')\n",
    "halo_batch_params = np.load(f'{TEMP_DIR}/halo_batch_{hname}_params.npy')\n",
    "halo_num = len(halo_batch_params)\n",
    "\n",
    "print('********Number density band********')\n",
    "print('Halo batch params (Rvir,Mvir,cNFW):')\n",
    "print(halo_batch_params)\n",
    "print('***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo 3/3 ; snapshot 0036\n"
     ]
    }
   ],
   "source": [
    "# =============================================== #\n",
    "# Run precalculations for selected halo in batch. #\n",
    "# =============================================== #\n",
    "\n",
    "halo_j = 2\n",
    "halo_ID = halo_batch_IDs[halo_j]\n",
    "\n",
    "# Generate progenitor index array for current halo.\n",
    "splits = re.split('/', SIM_TYPE)\n",
    "MTname = f'{PRE.SIM}_{splits[0]}_{splits[1]}'\n",
    "proj_IDs = fct.read_MergerTree(PRE.OUT_DIR, MTname, halo_ID)\n",
    "\n",
    "# Create empty arrays to save specifics of each loop.\n",
    "save_GRID_L = np.zeros(len(PRE.NUMS_SNAPS))\n",
    "save_num_DM = np.zeros(len(PRE.NUMS_SNAPS))\n",
    "\n",
    "j = 0\n",
    "snap = PRE.NUMS_SNAPS[::-1][0]\n",
    "proj_ID = proj_IDs[0]\n",
    "proj_ID = int(proj_ID)\n",
    "\n",
    "# Output halo progress.\n",
    "print(f'halo {halo_j+1}/{halo_num} ; snapshot {snap}')\n",
    "\n",
    "\n",
    "# --------------------------- #\n",
    "# Read and load DM positions. #\n",
    "# --------------------------- #\n",
    "\n",
    "IDname = f'origID{halo_ID}_snap_{snap}'\n",
    "fct.read_DM_halo_index(\n",
    "    snap, proj_ID, IDname, PRE.SIM_DIR, TEMP_DIR\n",
    ")\n",
    "DM_raw = np.load(f'{TEMP_DIR}/DM_pos_{IDname}.npy')\n",
    "DM_particles = len(DM_raw)\n",
    "\n",
    "\n",
    "# ---------------------- #\n",
    "# Cell division process. #\n",
    "# ---------------------- #\n",
    "\n",
    "# Initialize grid.\n",
    "snap_GRID_L = (int(np.abs(DM_raw).max()) + 1)*kpc\n",
    "raw_grid = fct.grid_3D(snap_GRID_L, snap_GRID_L)\n",
    "init_grid = np.expand_dims(raw_grid, axis=1)\n",
    "\n",
    "# Prepare arrays for cell division.\n",
    "DM_raw *= kpc\n",
    "DM_pos = np.expand_dims(DM_raw, axis=0)\n",
    "DM_pos_for_cell_division = np.repeat(DM_pos, len(init_grid), axis=0)\n",
    "del DM_raw\n",
    "\n",
    "# Cell division.\n",
    "cell_division_count = fct.cell_division(\n",
    "    init_grid, DM_pos_for_cell_division, snap_GRID_L, PRE.DM_LIM, None, TEMP_DIR, IDname\n",
    ")\n",
    "del DM_pos_for_cell_division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunksize_short_range(cells, DM_tot, max_DM_lim, core_mem_MB):\n",
    "\n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_short_range.\n",
    "    # -> Peak memory after calculation of ind_2D,ind_3D,etc. sorting arrays.\n",
    "\n",
    "    elem = 8                               # 8 bytes for standard np.float64\n",
    "    mem_type0 = cells*3 * elem             # for list to ndarray of cell_coords\n",
    "    mem_type1 = cells*DM_tot * elem        # for ind_2D\n",
    "    mem_type2 = cells*DM_tot*3 * elem      # for DM_pos_sync, ind_3D, DM_sort\n",
    "    mem_type3 = cells*max_DM_lim*3 * elem  # for DM_in\n",
    "\n",
    "    mem_MB = (mem_type0+mem_type1+(3*mem_type2)+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "\n",
    "\n",
    "def batch_generators_short_range(cell_coords, cell_gen, chunksize):\n",
    "\n",
    "    cells = len(cell_coords)\n",
    "\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "    batch_arr = np.arange(batches)\n",
    "\n",
    "    cell_chunks = chunks(chunksize, cell_coords)\n",
    "    cgen_chunks = chunks(chunksize, cell_gen)\n",
    "    \n",
    "    return batch_arr, cell_chunks, cgen_chunks\n",
    "\n",
    "\n",
    "def cell_gravity_short_range(\n",
    "    cell_coords_in, cell_gen, init_GRID_S,\n",
    "    DM_pos, DM_lim, DM_sim_mass, smooth_l,\n",
    "    out_dir, b_id\n",
    "):\n",
    "\n",
    "    cell_coords = np.expand_dims(np.array(cell_coords_in), axis=1)\n",
    "    cell_gen = np.array(cell_gen)\n",
    "\n",
    "    # Center all DM positions w.r.t. cell center.\n",
    "    # DM_pos already in shape = (1, DM_particles, 3)\n",
    "    DM_pos_sync = np.repeat(DM_pos, len(cell_coords), axis=0)\n",
    "    DM_pos_sync -= cell_coords\n",
    "\n",
    "    # Cell lengths to limit DM particles. Limit for the largest cell is \n",
    "    # GRID_S/2, not just GRID_S, therefore the cell_gen+1 !\n",
    "    cell_len = np.expand_dims(init_GRID_S/(2**(np.array(cell_gen)+1)), axis=1)\n",
    "\n",
    "    # Select DM particles inside each cell based on cube length generation.\n",
    "    DM_in_cell_IDs = np.asarray(\n",
    "        (np.abs(DM_pos_sync[:,:,0]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,1]) < cell_len) & \n",
    "        (np.abs(DM_pos_sync[:,:,2]) < cell_len)\n",
    "    )\n",
    "    del cell_gen, cell_len\n",
    "\n",
    "    # Set DM outside cell to nan values.\n",
    "    DM_pos_sync[~DM_in_cell_IDs] = np.nan\n",
    "    del DM_in_cell_IDs\n",
    "\n",
    "    # Sort all nan values to the bottom of axis 1, i.e. the DM-in-cell-X axis \n",
    "    # and truncate array based on DM_lim parameter. This simple way works since \n",
    "    # each cell cannot have more than DM_lim.\n",
    "    ind_2D = DM_pos_sync[:,:,0].argsort(axis=1)\n",
    "    ind_3D = np.repeat(np.expand_dims(ind_2D, axis=2), 3, axis=2)\n",
    "    DM_sort = np.take_along_axis(DM_pos_sync, ind_3D, axis=1)\n",
    "    DM_in = DM_sort[:,:DM_lim*SHELL_MULTIPLIERS[-1],:]\n",
    "\n",
    "    # note: Memory peaks here, due to these arrays:\n",
    "    # print(DM_pos_sync.shape, ind_2D.shape, ind_3D.shape, DM_sort.shape, DM_in.shape)\n",
    "    # mem_inc = gso(cell_coords)+gso(DM_pos_sync)+gso(ind_2D)+gso(ind_3D)+gso(DM_sort)+gso(DM_in)\n",
    "    # print('MEM_PEAK:', mem_inc/1e6)\n",
    "    del DM_pos_sync, ind_2D, ind_3D, DM_sort\n",
    "\n",
    "    # Calculate distances of DM and adjust array dimensionally.\n",
    "    DM_dis = np.expand_dims(np.sqrt(np.sum(DM_in**2, axis=2)), axis=2)\n",
    "\n",
    "    # Offset DM positions by smoothening length of Camila's simulations.\n",
    "    eps = smooth_l / 2.\n",
    "    # eps = smooth_l\n",
    "\n",
    "    # nan values to 0 for numerator, and 1 for denominator to avoid infinities.\n",
    "    quot = np.nan_to_num(cell_coords - DM_in, copy=False, nan=0.0) / \\\n",
    "        np.nan_to_num(\n",
    "            np.power((DM_dis**2 + eps**2), 3./2.), copy=False, nan=1.0\n",
    "        )\n",
    "    \n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative = -G*DM_sim_mass*np.sum(quot, axis=1)    \n",
    "    np.save(f'{out_dir}/batch{b_id}_short_range.npy', derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from cell division.\n",
    "fin_grid = np.load(f'{TEMP_DIR}/fin_grid_{IDname}.npy')\n",
    "DM_count = np.load(f'{TEMP_DIR}/DM_count_{IDname}.npy')\n",
    "cell_com = np.load(f'{TEMP_DIR}/cell_com_{IDname}.npy')\n",
    "cell_gen = np.load(f'{TEMP_DIR}/cell_gen_{IDname}.npy')\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "# Calculate gravity grid (in batches of cells). #\n",
    "# --------------------------------------------- #\n",
    "cell_coords = np.squeeze(fin_grid, axis=1)\n",
    "cells = len(cell_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = PRE.MEM_LIM_GB*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / PRE.PRE_CPUs\n",
    "\n",
    "# Determine short-range chuncksize based on available memory and cells.\n",
    "chunksize_sr = chunksize_short_range(\n",
    "    cells, DM_particles, PRE.DM_LIM*SHELL_MULTIPLIERS[-1], core_mem_MB\n",
    ")\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "batch_arr, cell_chunks, cgen_chunks = batch_generators_short_range(\n",
    "    cell_coords, cell_gen, chunksize_sr\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# cell_gravity_short_range(\n",
    "#     next(cell_chunks), next(cgen_chunks), snap_GRID_L, DM_pos, \n",
    "#     PRE.DM_LIM, PRE.DM_SIM_MASS, \n",
    "#     PRE.SMOOTH_L, TEMP_DIR, batch_arr[0]\n",
    "# )\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "    ex.map(\n",
    "        cell_gravity_short_range, \n",
    "        cell_chunks, cgen_chunks, repeat(snap_GRID_L), repeat(DM_pos), \n",
    "        repeat(PRE.DM_LIM), repeat(PRE.DM_SIM_MASS), \n",
    "        repeat(PRE.SMOOTH_L), repeat(TEMP_DIR), batch_arr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 3)\n"
     ]
    }
   ],
   "source": [
    "# Combine short-range batch files.\n",
    "dPsi_short_range_batches = [\n",
    "    np.load(f'{TEMP_DIR}/batch{b}_short_range.npy') for b in batch_arr\n",
    "]\n",
    "dPsi_short_range = np.array(\n",
    "    list(chain.from_iterable(dPsi_short_range_batches))\n",
    ")\n",
    "np.save(\n",
    "    f'{TEMP_DIR}/dPsi_short_range_{IDname}.npy', \n",
    "    dPsi_short_range\n",
    ")\n",
    "\n",
    "gravity_sr = np.load(f'{TEMP_DIR}/dPsi_short_range_{IDname}.npy')\n",
    "print(gravity_sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183,) 4.635577750162663e-34\n"
     ]
    }
   ],
   "source": [
    "mags_sr = np.sqrt(np.sum(gravity_sr**2, axis=1))\n",
    "print(mags_sr.shape, np.max(mags_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-range gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division rounds: 4\n",
      "SHAPES: (183, 3) (183,) (183, 3)\n",
      "************\n",
      "183\n",
      "Before 697\n",
      "After 0\n",
      "(183, 3)\n",
      "(183,) 8.250400909984426e-36\n",
      "[-4.94299072e-35 -4.74802435e-35  1.53854474e-35]\n"
     ]
    }
   ],
   "source": [
    "def chunksize_long_range(cells, core_mem_MB):\n",
    "    \n",
    "    # note: mem_MB specific to peak memory usage in cell_gravity_long_range.\n",
    "    # -> Peak memory after calculation of derivative.\n",
    "\n",
    "    elem = 8                          # 8 bytes for standard np.float64\n",
    "    mem_type1 = 3*elem                # for derivative\n",
    "    mem_type2 = cells*3*elem          # for quot\n",
    "    mem_type3 = cells*elem            # for DM_count_sync\n",
    "\n",
    "    mem_MB = (mem_type1+mem_type2+mem_type3)/1.e6\n",
    "\n",
    "    batches = 1\n",
    "    while mem_MB >= 0.95*core_mem_MB:\n",
    "        mem_MB *= batches\n",
    "        batches += 1\n",
    "        mem_MB /= batches\n",
    "\n",
    "    chunksize = math.ceil(cells/batches)\n",
    "\n",
    "    return chunksize\n",
    "    \n",
    "\n",
    "def batch_generators_long_range(\n",
    "    cell_coords, com_coords, DM_counts,\n",
    "    chunksize \n",
    "):\n",
    "    cells = len(cell_coords)\n",
    "    cell_nums = np.arange(cells)\n",
    "\n",
    "    batches = math.ceil(cells/chunksize)\n",
    "\n",
    "    # Arrays used for naming files.\n",
    "    id_arr = np.array([idx+1 for idx in cell_nums for _ in range(batches)])\n",
    "    batch_arr = np.array([b+1 for _ in cell_nums for b in range(batches)])\n",
    "\n",
    "    # Coord of cell, for which long-range gravity gets calculated.\n",
    "    coord_arr = np.array([cc for cc in cell_coords for _ in range(batches)])\n",
    "\n",
    "    # Chunks for DM_count array, as a generator for all cells.\n",
    "    count_gens = (c for _ in cell_nums for c in chunks(chunksize, DM_counts))\n",
    "    count_chain = chain(gen for gen in count_gens)\n",
    "\n",
    "    # Chunks for cell_com array, as a generator for all cells.\n",
    "    com_gens = (c for _ in cell_nums for c in chunks(chunksize, com_coords))\n",
    "    com_chain = chain(gen for gen in com_gens)\n",
    "\n",
    "    return id_arr, batch_arr, coord_arr, count_chain, com_chain\n",
    "    \n",
    "\n",
    "def cell_gravity_long_range(\n",
    "    c_id, b_id, cellX_coords, \n",
    "    DM_count, cell_com, \n",
    "    DM_sim_mass, smooth_l, out_dir\n",
    "):\n",
    "\n",
    "    # Distances between cell centers and cell c.o.m. coords.\n",
    "    com_dis = np.expand_dims(\n",
    "        np.sqrt(np.sum((cellX_coords-cell_com)**2, axis=1)), axis=1\n",
    "    )\n",
    "\n",
    "    # Offset DM positions by smoothening length of Camila's simulations.\n",
    "    eps = smooth_l / 2.\n",
    "    # eps = smooth_l\n",
    "\n",
    "    # Long-range gravity component for each cell (including itself for now).\n",
    "    quot = (cellX_coords-cell_com)/np.power((com_dis**2 + eps**2), 3./2.)\n",
    "    DM_count_sync = np.expand_dims(DM_count, axis=1)\n",
    "    del com_dis\n",
    "\n",
    "    # Set self-gravity to zero.\n",
    "    # print(DM_count_sync.shape)\n",
    "    # print(c_id)\n",
    "    print('Before', DM_count_sync[c_id-1, 0])\n",
    "    DM_count_sync[c_id-1, 0] = 0.\n",
    "    print('After', DM_count_sync[c_id-1, 0])\n",
    "    # print(quot.shape)\n",
    "\n",
    "    strength = -G*DM_sim_mass*DM_count_sync*quot\n",
    "    print(strength[c_id-1])\n",
    "    mags = np.sqrt(np.sum(strength**2, axis=1))\n",
    "    print(mags.shape, np.max(mags))\n",
    "\n",
    "    # note: Minus sign, s.t. velocity changes correctly (see GoodNotes).\n",
    "    derivative = -G*DM_sim_mass*np.sum(DM_count_sync*quot, axis=0)\n",
    "    print(derivative)\n",
    "\n",
    "    # note: Memory peaks here, due to these arrays:\n",
    "    # print(quot.shape, DM_count_sync.shape, derivative.shape)\n",
    "    # mem_inc = gso(quot)+gso(DM_count_sync)+gso(derivative)\n",
    "    # print(mem_inc/1e6)\n",
    "    del quot, DM_count_sync\n",
    "\n",
    "    np.save(f'{out_dir}/cell{c_id}_batch{b_id}_long_range.npy', derivative)\n",
    "\n",
    "\n",
    "def load_dPsi_long_range(c_id, batches, out_dir):\n",
    "    \n",
    "    # Load all batches for current cell.\n",
    "    dPsi_raw = np.array(\n",
    "        [np.load(f'{out_dir}/cell{c_id}_batch{b}_long_range.npy') for b in batches]\n",
    "    )\n",
    "\n",
    "    dPsi_for_cell = np.sum(dPsi_raw, axis=0)\n",
    "    np.save(f'{out_dir}/cell{c_id}_long_range.npy', dPsi_for_cell)  \n",
    "\n",
    "\n",
    "# Load files from cell division.\n",
    "fin_grid = np.load(f'{TEMP_DIR}/fin_grid_{IDname}.npy')\n",
    "DM_count = np.load(f'{TEMP_DIR}/DM_count_{IDname}.npy')\n",
    "cell_com = np.load(f'{TEMP_DIR}/cell_com_{IDname}.npy')\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "# Calculate gravity grid (in batches of cells). #\n",
    "# --------------------------------------------- #\n",
    "cell_coords = np.squeeze(fin_grid, axis=1)\n",
    "cells = len(cell_coords)\n",
    "\n",
    "print('Division rounds:', cell_division_count)\n",
    "print('SHAPES:', cell_coords.shape, DM_count.shape, cell_com.shape)\n",
    "print('************')\n",
    "\n",
    "# Calculate available memory per core.\n",
    "mem_so_far = (psutil.virtual_memory().used - OS_MEM)/MB_UNIT\n",
    "mem_left = PRE.MEM_LIM_GB*1e3 - mem_so_far\n",
    "core_mem_MB = mem_left / PRE.PRE_CPUs\n",
    "\n",
    "# Determine long-range chuncksize based on available memory and cells.\n",
    "chunksize_lr = chunksize_long_range(cells, core_mem_MB)\n",
    "\n",
    "# Split workload into batches (if necessary).\n",
    "cell_ids, batches, coords, count_chain, com_chain = batch_generators_long_range(\n",
    "    cell_coords, cell_com, DM_count, chunksize_lr\n",
    ")\n",
    "\n",
    "\n",
    "# cell_gravity_long_range(\n",
    "#     cell_ids[0], batches[0], \n",
    "#     coords[0], next(count_chain), next(com_chain), \n",
    "#     PRE.DM_SIM_MASS, PRE.SMOOTH_L, TEMP_DIR\n",
    "# )\n",
    "\n",
    "last_count = list(count_chain)[-1]\n",
    "last_com = list(com_chain)[-1]\n",
    "\n",
    "print(cell_ids[-1])\n",
    "\n",
    "cell_gravity_long_range(\n",
    "    cell_ids[-1], batches[-1], \n",
    "    coords[-1], last_count, last_com, \n",
    "    PRE.DM_SIM_MASS, PRE.SMOOTH_L, TEMP_DIR\n",
    ")\n",
    "\n",
    "\n",
    "# with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "#     ex.map(\n",
    "#         cell_gravity_long_range, cell_ids, batches, \n",
    "#         coords, count_chain, com_chain,\n",
    "#         repeat(PRE.DM_SIM_MASS), repeat(PRE.SMOOTH_L), repeat(TEMP_DIR)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 3) [3.49735510e-34 2.53571754e-34 5.40151363e-34]\n",
      "(183,) 7.177394606954965e-34\n"
     ]
    }
   ],
   "source": [
    "# Combine long-range batch files.\n",
    "load_batch_arr = np.unique(batches)\n",
    "with ProcessPoolExecutor(PRE.PRE_CPUs) as ex:\n",
    "    ex.map(\n",
    "        load_dPsi_long_range, cell_ids, \n",
    "        repeat(load_batch_arr), repeat(TEMP_DIR)\n",
    "    )\n",
    "\n",
    "dPsi_long_range = np.array([\n",
    "    np.load(f'{TEMP_DIR}/cell{c}_long_range.npy') for c in cell_ids\n",
    "])\n",
    "np.save(\n",
    "    f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy', \n",
    "    dPsi_long_range\n",
    ")\n",
    "\n",
    "gravity_lr = np.load(f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy')\n",
    "print(gravity_lr.shape, np.max(gravity_lr, axis=0))\n",
    "\n",
    "mags_lr = np.sqrt(np.sum(gravity_lr**2, axis=1))\n",
    "print(mags_lr.shape, np.max(mags_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine long-range batch files.\n",
    "load_batch_arr = np.unique(batches)\n",
    "load_dPsi_long_range( \n",
    "    cell_ids[0], load_batch_arr, TEMP_DIR\n",
    ")\n",
    "\n",
    "dPsi_long_range = np.array([np.load(f'{TEMP_DIR}/cell1_long_range.npy')])\n",
    "np.save(\n",
    "    f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy', \n",
    "    dPsi_long_range\n",
    ")\n",
    "\n",
    "gravity_lr = np.load(f'{TEMP_DIR}/dPsi_long_range_{IDname}.npy')\n",
    "print(gravity_lr.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
